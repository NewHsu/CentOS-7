## LAW 

### 理解调优关键理论（队列理论）

* 在软件的调试过程中，会使用不同类型的钩子或者工具来收集其性能方面的数据，但是往往收集到的数据之间的关系错综复杂以及需要非常专业的知识才可进行分析。
* 如果你发现这个过程非常复杂，并且难以理解，那么可以换个角度去看，以一个专业的理论来作为对数据的支撑，将数据嵌套到这个理论中，充分理解调优的关键点。
* 调优过程中最有指导意义的理论，就是队列理论（**queuing theory** ）。

### 队列理论的核心思想

* 1961年由数学家John Little以数学的形式表现出来，便于技术人员实现性能管理以及对性能调优建立量化的衡量与操作标准 。

* 该理论可以让我们 “以工程学的方法来进行性能管理”、“量化系统未来性能”、“充分理解和说明监控工具的输出内容之间关系”、“验证测量值和软件数据的正确性” 等等实际并且有指导意义的分析和验证及调试的基础理论。

* 队列公式 ： 

  * **L = A *  W  （队列长度 = 到达率 * 等待时间）**
  * L = 队列长度（Queue Length）系统中等待处理的请求数
  * A = 到达率（Arrival Rate）若以秒为单位，则是每秒有多少请求到达（选定恒定的观察时间）
  * W = 等待时间（Wait Time）处理一个请求所需要的时间，等同于延迟、响应时间或者驻留时间

* 作为Little法则的结果，等待时间可能会比观察时间长，因为他体现了所有在队列中等待资源的请求所需要的时间。如果在观察时间之内请求产生的数量多于资源能够处理的数量，那么队列大小将增长并且等待时间会增加。

* 而到达率是到达系统的请求的比例，**A=arrivals/observation period A=到达率/观测时间**，观测时间是恒定的（必须约定一个观测时间，是1秒还是1分钟）

* L=A*W的关系可以在iostat中得到体现。一般iostat可以告知每秒钟读写的量以及请求数量。

  ```
  [root@localhost ~]# iostat -d /dev/sda 1 10 -k
  Linux 3.10.0-693.el7.x86_64 (localhost.localdomain) 2018年06月27日 _x86_64_	(1 CPU)
  
  Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
  sda               1.02        24.27         5.32     406644      89069
  ```

### 基于队列理论的调优思想

#### 队列长度（L）

* 队列是可以调的读写或者只读队列，往往读的队列要在写队列之前执行，因为计算机的读比写处理占比要大的多，所以为了防止队列出现饥饿现象，则必须用**算法**进行维持和调度，并且队列有优先级的概念。

* 队列长短对系统有什么影响？

  * 短队列：不利于请求合并处理，会频繁产生请求，尤其是IO的时候将会效率底下，但是内存压力会很小。
  * 长队列：有利于请求进行合并处理，将会使得请求效率更高，但是内存压力非常大。

* 如何查看系统中的队列？

  * 由于队列是可以调节的，所以可以使用root用户来查找并修改。（不建议修改，通过其他方式进行调整）

  ```
  find /proc /sys -type f -writable -ls
  ```

* 队列的算法实际上有何意义？

  * 一般的队列是基于优先级来实现并有两种实现方式。

    * 一：

      * 针对一个资源建立多个队列，例如：对一个磁盘的访问建立两条队列，一条队列用于紧急任务而另一条队列用于一般任务。队列算法假设为先处理紧急队列任务，然后处理一般队列任务。
      * 一般队列任务只有等到紧急任务处理完之后才能进行处理。这种算法的缺点显而易见，如果紧急任务的队列永远都处于非空状态，那么一般任务的队列将进入饥饿状态（永远得到不处理），因此真正的队列算法应该避免这种情况的出现。

    * 二：

      * 只使用一个队列，这种算法会在队列的头部合并一些紧急任务。而且这种算法必须通过一定的逻辑性来防止队列饥饿。例如：处理读请求的队列负载大大超过写请求的情况下，磁盘写入的操作可能被视为非紧急任务而被延迟。而事实上很多的真实队列算法会合并读写两种操作到队列中。

      > 所以队列的优化实际就是算法的优化，选择合适的算法将事半功倍。

  * **是什么影响队列长度？**

    * 其实这个问题的关键点在于L = A * W 中，L的大小取决于 A 和 W，但是我们能控制的只有W 和 L（上文所述，队列大小是可以调节的），因为A（到达率）是我们无法掌控的，这就比如，来超市买东西的顾客一样，你永远不知道今天回来多少顾客，甚至什么时间顾客是最多的，而顾客将相当于A（到达率）。

    * L =  A * W 是一个平衡的公式，无论哪个值改变都将会产生变化，简单来举一个队列增加和减少的实例：

      * 超市的收银员相当于处理系统，顾客相当于请求，我们通过不同的变化来理解队列长度问题，那么现在开始运营。

      * 假设到达率为 20个顾客/分钟，每个顾客的处理的时间是2分钟，则公式为：

        ```
        40 req = 20 req/m * 2m(分钟)，队列长度为40
        ```

      - 试着提升处理速度变为1分钟

        ```
        20 req = 20 req/m * 1m , 对列长度为20，提升了W，将缩短队列长度。
        ```

      * 试着降低处理速度变为4分钟

        ```
        80 req = 20 req/m * 4m , 队列长度变为80，降低了W，将增加队列长度。
        ```

    * 根据上面的实例，可以充分证明，等待时间和队列的长度有密切的关系，如果其中一个值发生变化，那么另一个值也会产生相应的变化。

    * 队列的长短对于性能是有直接影响的，例如：缩短队列，则可能降低队列对内存的压力和负载，但是可能会使得读取的数据无法排序而造成后续写性能的下降，而如果队列加长也有其好处，尤其针对硬盘的操作可以在长队列的情况下对队列数据排序并优化。 

  * 队列中的读写请求，优先级该如何取舍？

    * 通常情况下，我们一直致力于提高读的性能，因为读的动作比写的动作更多，但是在这种情况下再去尝试优化写。所以应该把握的原则是读比写优先。 

    > Linux系统中，存在分类型队列的概念，即有多个读或者写队列，分别有不同的优先级，以实现对不同的数据进行优化。 

  #### 等待时间（W）

  * 通过理解队列长度，我们会发现，影响队列长度的实际因素是A 和 W，但是A（到达率）我们有没有办法掌控，所以W将成为我们调优的关键点。

  * W（等待时间）中包含哪些时间？

    *  Queue time（排队时间）：请求资源所花费的时间
    *  Service time (服务时间）：处理一个请求所花的时间

  * 根据上面的说法，公式是不是产生了变化？

    * 公式可以变换为 ： **L = A * (Queue time + Service time)**
    * 这个公式也给我们指了一个方向：**减少排队时间** 和 **减少服务时间**

  * Linux 系统中如何查看W的时间？

    * 通过time命令来获取一个操作所花费的时间，其中包括用户时间（User Time）和系统时间（Sys Time），但是用户时间加上系统时间往往不等于real time，因为其中会有一些时间是属于排队时间。

    * 任何一个进程在试图和kernel交互的时候会block，例如在读写硬盘数据的时候，如果硬盘太慢，则会sleep，当可以处理的时候进程方可被唤醒。

      ```
      real	0m0.032s   -->实际完成时间
      user	0m0.002s   -->用户时间
      sys		0m0.027s   -->系统时间
      ```

    * 所以公式可以改成：**L = A * （ Queue Time + User Time + Sys Time )** 

    * 因此 **Service Time = User Time + Sys Time **

  * 如何通过Service Time 判定效率问题？

    * 单纯从Service Time来判定是否有效率问题，这个有点偏激，但是Service Time确实给我们了一个分析的方向。
    * Linux系统中任何进程在系统执行的时候都有用户时间（User Time）和系统时间（Sys Time），一般用户时间是真正在cpu上工作的时间，而系统时间是花在kernel处理上的时间。如果一个进程大量花费系统时间，证明进程的效率比较低，可能是将大量时间花费在中断和IO上。所以我们的原则是尽量将系统时间缩短。
    * 所以高效的User Time 将说明这事一个对cpu具有高使用量的程序。

  #### 如何降低W（等待时间）

  * 如何才能找到症结所在，来优化等待时间？其实说的直白一点，就是通过什么方法来找到程序的执行热点。

  * 惯用手段如下：
    - strace ： 是一个非常简单的工具，它可以跟踪系统调用的执行。可以从头到尾跟踪binary的执行，然后以一行文本输出系统调用的名字，参数和返回值。

  * **重要概念（system call）**
    * 任何一个用户层程序试图访问kernel层（ring 0）的时候的过程称为system call，任何应用程序和内核交互的时候都要试图访问kernel都要通过system call（例如，用户层应用程序需要访问或者使用硬件资源，这时候，用户层是没有权限直接使用的，需要经由Kernel层调用后才能使用），但是直接访问system call将很麻烦，所以我们通过glibc来和system call进行交互。 

    * strace 用法实例：（查找SSH链接反应缓慢的问题）

    ​	  问题描述：

    ​		  用ssh连局域网内其他linux机器，会等待10-30秒才有提示输入密码。严重影响工作效率。

    ​	  解决办法：（其实很多解决办法，这里选用strace来进行）

    ​		  在服务端使用strace监控sshd服务，并且将输出内容重定向，然后进行分析。

    ```
    [root@ceph-1 ~]# strace -o sshd1317.strace -fT -p 1317     <--服务端监听sshd服务
    
    -o 输出到文件，后接文件名
    -f 跟踪由fork调用所产生的子进程.
    -T 显示每一调用所耗的时间.
    -p 进行号
    
    [root@ceph-1 ~]# vim sshd1317.strace
    
    2659  socket(AF_INET, SOCK_DGRAM|SOCK_NONBLOCK, IPPROTO_IP) = 4 <0.000014>
    2659  connect(4, {sa_family=AF_INET, sin_port=htons(53), sin_addr=inet_addr("192.168.56.11")}, 16) = 0 <0.000018>
    2659  poll([{fd=4, events=POLLOUT}], 1, 0) = 1 ([{fd=4, revents=POLLOUT}]) <0.000009>
    2659  sendto(4, "C\207\1\0\0\1\0\0\0\0\0\0\003151\00256\003168\003192\7in-a"..., 45, MSG_NOSIGNAL, NULL, 0) = 45 <0.000217>
    2659  poll([{fd=4, events=POLLIN}], 1, 5000 <unfinished ...>
    2660  read(4,  <unfinished ...>
    2659  <... poll resumed> )              = 0 (Timeout) <5.001764> 
    ...
    
    分析关键点内容，即调用socket通信的时候将信息发给了192.168.57.11，然后调用poll，结果超时 (Timeout) <5.007096>，以供三次循环，导致链接的时候出现缓慢，分析其原因，是发往DNS服务器之后没有响应导致。
    ssh连接的时候sshd服务是向DNS服务器进行了反解，所以我们只要到对应的sshd_conf配置文件中关闭“USEDNS”即可。问题解决。
    这里给出的思维是利用strace进行问题深入分析，也是测底查看连接的时候SSHD服务到底做了什么，导致连接响应时间过长。(一定要有举一反三的思路)
    ```

    * strace 用法实例2：（分析利用率）

    ​	问题描述：

    ​		想知道程序的执行效率

    ​	解决办法：

    ​		可以监控正在执行中的程序，也可以监控从开始到结束

    ```
    [root@localhost /]# strace -c ls -R >/dev/null     
    -c 计每一系统调用的所执行的时间,次数和出错的次数等.
    % time     seconds  usecs/call     calls    errors syscall
    ------ ----------- ----------- --------- --------- ----------------
     50.31    0.128319           3     45627           getdents
     29.29    0.074705           3     22805           openat
     11.58    0.029531           1     22819           close
      8.30    0.021161           1     22818           fstat
     ......
    ------ ----------- ----------- --------- --------- ----------------
    100.00    0.255062                115430         7 total
    
    这组数据可以分析ls这个命令执行的时候调用时间占比，大多数调用是在getdents和openat，符合ls的指令逻辑。它耗费了大部分时间在45627次调用来读取目录条目上，因为我们在根目录下，加上-R参数，目录结构比较庞大。
    ```

    > Strace 有非常多的参数可以用来使用，让我们可以更透彻的了解程序的运行状态，从而对于调优有更直接的数据进行分析。
    >
    > 详细的strace参数可以参考 “man strace”

#### 完成率

* 现在我们来学习另外一个重要的概念和公式“完成率”，在开始之前，我们先熟悉几个关键术语并理解公式的意义。

##### 术语

* 带宽（ Bandwidth  ）
  * 带宽是指在给定时间之内通过的固定大小为单位的数据单位总量 。
  * 真正的带宽是指在同等条件下实际 通过的固定大小单位的数据总量。 
* 吞吐量（ hroughput  ）
  * 吞吐量指在给定时间内通过的有用的数据量 。
* 开销（ Overhead ）
  * 指的是传输有用数据时产生的消耗 。
* 完成率（Complete Rate） 
  * 处理成功的请求数量。

##### 公式和关系

* 带宽 = 吞吐量 + 开销
* 带宽固定的情况下，开销越小，通过的有用数据越多，即完成量越高，所以吞吐量理论上来讲即是完成量。
* 根据上面的理论，可以推导出，只要减小“开销”，就可以高“完成量”。
* 单位时间的完成量（X）= 完成总量（Completions) / 观察时间（observation period ）

##### 如何提高完成率

* 降低开销和使用更高效的算法
* 比如网络传输，TCP首部开销20字节;UDP的首部开销小，只有8个字节，所以采用udp效率高，但是某些时候我们需要TCP的确认和重传机制，UDP无法实现，这时候就要依据实际环境来判断了，比如视频类的我们可以采用udp，丢几个包也无所谓，高精度的业务系统就要采用TCP，尽可能减少丢包率。

##### 完成量和队列

* 实际应用理论的时候，我们应该更多的关注“完成量和队列”的关系，以实现最佳的平衡效果。
* 调优最好的效果就是来一个请求，处理一个请求，不会产生队列和积压，但是这个目标显然很不现实。
* 就如文章上面所讲的，我们无法控制到达率，因为到达率不是恒定的，所以肯定会有高峰和低峰，如果处理速度恒定，那么就会产生处理饥饿或者因为高峰期处理而产生处理队列。
* 最佳状态即：到达率（A）平均值 = 完成量（C）平均值 ，这也是最终的调优目标。

## 基于该理论的调优建议

* 调节L：
  * 调整队列适应读操作，并且限制合适的长度。
* 优化A：
  * 分散到达率，使用负载均衡技术将到达率尽可能的平均分散处理。
  * 采用缓存机制来降低对主要资源的直接访问。
  * 采用开销更小的协议提高效率。
* 优化W：
  * 采用更小服务时间的设备，即更快的响应处理设备。
  * 每个资源请求给予更小的请求时间。

## 总结
LAW队列理论是调优的基础理论，深入理解队列理论对于调优或者故障排除都有很好的帮助，关键点就是LAW的公式。


### 通过该命令显示的结果来说明几个调优的时候需要掌握的原则：
[root@dhcp-0-195 data1]# dd if=/dev/zero of=test.img bs=1M count=1024 &
[1] 10053
[root@dhcp-0-195 data1]# iostat -x 1 20
Linux 2.6.18-8.1.1.el5 (dhcp-0-195.pek.redhat.com)      2008?07?18?

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.34    0.01    0.06    0.13    0.00   99.47

Device:         rrqm/s   wrqm/s   r/s   w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda               0.35     2.54  1.11  0.50    37.12    22.66    37.13     0.03   17.20   1.87   0.30
dm-0              0.00     0.00  0.75  1.24    18.83     9.95    14.47     0.10   49.22   0.98   0.19

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.00    0.00   14.50   81.00    0.00    4.50

Device:         rrqm/s   wrqm/s   r/s   w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda               0.00  4815.15  4.04 103.03    40.40 101155.56   945.13   127.26 1521.19   9.44 101.11
dm-0              0.00     0.00  3.03  3.03    32.32    24.24     9.33     3.70 1623.83 149.00  90.30

Little’s LAW：L = A x W
L = 队列长度（Queue Length）		等待系统处理的平均请求数量
A = 到达率（数量）（Arrival Rate）	进入系统中的工作量
W = 等待时间（Wait Time）		residence time驻留时间

A为（r/s + w/s）
完成数量（Completion Rate），即每秒处理的扇区数（rsec/s + wsec/s）
平均队列长度为avgqu-sz
W（等待时间Wait Time或者Average Residence Time）为await，相当于队列时间
平均服务时间为svctm

所以L = （r/s + w/s）* await / 1000 msec	--获得的是队列数


Utilization LAW：utilization = （service time） * （arrival rate）
在不稳定的系统中，complete rate （resc/s + wsec/s）= arrival rate （r/s + w/s）
对于饱和的资源：
最大的A = 1 / （service time），即 A（r/s +ｗ/s）= 1 / svctm
那么
U = A * svctm / 1000 msec


系统整体的吞吐量由系统中最慢的设备决定。
Xsystem = Xresource / Vresource
Vresource 访问数量
Xresource 吞吐量


### 参考
第五章实验：

实验一：和队列原理有关的问题：

第一个问题是：
有管理员报告这样的问题：他在不同的两台服务器上测试磁盘读写性能，一台服务器配置比较好，而另外一台服务器配置比较差。但是他观察到的读写速度基本上都一样。

测试的方法是：

# dd if=/dev/zero of=/dev/null

问题在哪里？

从测试方法来看，/dev/zero中读取的数据永远无穷，而/dev/null中的数据永远被丢弃。实际上这两个设备都不是磁盘子系统中的真实设备，因此磁盘子系统并没有真正参与到测试中。而且这种测试方法无法体现出多CPU的优势，因此测试出来的结果基本相等。

第二个问题是：
你的老板希望节省费用，在你所管理的服务器上部署一个磁盘应用比较多的程序。在通过iostat查看该程序运行情况的时候发现，该磁盘上每秒钟大概有290个读清求，而写请求是0个，平均服务时间是3.36毫秒。那么如何对你的老板解释这种情况：

该case的难点是，如何确定该磁盘已经达到了资源使用的极限？

计算方法：

    (290 + 0) requests | 3.36 ms   |1s
U = -------------------+-----------+----------- = 0.9744
              s        | request   | 1000 ms

可以测出磁盘使用率为0.9744，

根据公式：磁盘利用率 ＝ 服务时间 * 到达率

如果我将到达率设置为1，即U＝1，则根据下面的公式，实际上到达的值应该是：

                 request | 1000 ms
arrival rate = ----------+---------- = 297.62 req/s
               3.36 ms   |1s

这就是说高于298的每秒钟读清求将导致等待时间增加。如果这种状况持续则会导致系统出现状况。

根据P125的利用率法则：
utilization = (service time) * (arrival rate)		--> U = S * A

那么后面这条计算方法是通过将使用率设置即U设置为1，来将到达律A提到峰值，在服务时间不边的情况下获得最大的到达律A。
或者换句话说：

最大到达律（max arrival rate） = 1 / 服务时间（service time）。

这里的到达率实际上是read/s + write/s的和。



 
实验二：使用vmstat：

在被观测的服务器上运行命令：

# vmstat 1 100

而同时在另外一个终端上执行：

# dd if=/dev/zero of=/dev/null

可以看到vmstat 1 100时，r列的变化：

[root@dhcp-129-162 ~]# vmstat 1 100
procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 0  0    176 230780  76592 420192    0    0     4    30   35  154  1  0 99  0  0
 0  0    176 230780  76592 420192    0    0     0     0 1006  130  0  0 100  0  0
 0  0    176 230780  76600 420184    0    0     0   436 1008  135  0  0 100  0  0
 0  0    176 230780  76600 420192    0    0     0     0 1004  138  0  0 100  0  0
 1  0    176 230648  76604 420236    0    0    52     0 1011  145 12 12 73  3  0
 1  0    176 230648  76604 420240    0    0     0     0 1005  137 39 61  0  0  0
 1  0    176 230648  76604 420240    0    0     0     0 1005  135 39 61  0  0  0
 1  0    176 230648  76604 420240    0    0     0     0 1005  140 41 59  0  0  0
 1  0    176 230648  76604 420240    0    0     0     0 1033  189 39 61  0  0  0
 1  0    176 230648  76612 420232    0    0     0    28 1008  142 41 59  0  0  0
 1  0    176 230648  76612 420232    0    0     0     0 1005  135 43 57  0  0  0
 0  0    176 230648  76612 420240    0    0     0     0 1014  142 31 40 30  0  0
 0  0    176 230648  76612 420240    0    0     0     0 1004  138  0  0 100  0  0


 