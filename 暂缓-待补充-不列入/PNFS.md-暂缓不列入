# pNFS  

## 1. pNFS 简介
* pNFS是并行网络文件系统，即NFSv4.1，与NFSv3不同的是，它将元数据与数据进行分离，允许客户端直接与数据服务器进行交互。这种机制解决了传统NFS的性能瓶颈问题，从而使得系统获得高性能和高扩展性的特性。
* pNFS支持多种协议(Block[FC], Object[OSD], Files[NFS])直接访问数据，需要对客户端功能进行扩展以支持不同的layout。

## 2. pNFS(Parallel NFS)

 * pNFS它将普通 NFS 的优势和并行输入输出（I/O）的高传输率结合起来。使用 pNFS 时，客户机也像以前一样可以从服务器共享文件系统，但数据不经过 NFS 服务器。相反，客户机系统将与数据存储系统直接连接，为大型数据传输提供许多并行的高速数据路径。在简短的初始化和握手（handshaking）过程之后，pNFS将不再阻碍传输速率。

### 2.1 pNFS的概念组织结构

 pNFS的概念组织结构图如下，顶部是计算集群的节点，比如大量便宜的、基于 Linux 的刀片服务器群。左边是 NFS 服务器。底部是一个大型的并行文件系统。 

![](../images/pNFS/2.png)

* 如同NFS，pNFS 服务器也是文件系统，并且保留和维护数据仓库中用于描述每个文件的标准元数据。pNFS 的客户机如同NFS客户机一样，挂载集群中的一个节点输出的文件系统。
* 如同NFS，每个节点都将文件系统看作本地的物理卷附加文件系统。元数据的更改通过网络传回给pNFS服务器。
* 不同于NFS，pNFS在Read或Write数据时是在节点和存储系统之间直接操作的，从数据事务中移除pNFS服务器为pNFS提供了明显的性能优势。
* pNFS即保留了NFS的所有优点，并且改善了性能和可伸缩性。

### 2.2 pNFS 的具体细节
* pNFS 是由 3 个协议构成的。

![](../images/pNFS/3.png)

* pNFS协议：用于在服务器和客户机节点之间传输文件元数据（layout）。
 
        可以理解为位图或者地图，描述如何在数据仓库之间分配文件。还包含这许可和其他文件属性。并在服务器中持久化这些数据后，存储系统仅需执行I/O。

* 存储访问协议：指定客户机从数据仓库访问数据的方式。

        每个存储访问协议都定义自己的layout形式，因为访问协议和数据组织必须保持一致。

* 控制协议：同步元数据服务器和数据服务器之间的状态。
 
        同步是对客户机是无感知的，例如重新组织介质上的文件。NFSv4.1 并没有规定控制协议；所以它有多种形式，这将给用户更多的选择。

### 2.3 访问流程

    1. 客户机为当前的文件请求一个layout。
    2. 客户机通过打开元数据服务器上的文件获得访问权。
    3. 客户机获得授权和layout之后，就可以直接从数据服务器访问信息。根据存储类型所需的存储访问协议，访问继续进行。
    4. 如果客户机更改了这个文件，则会相应地更改布局的客户机实例，并且将所有更改提交回到元数据服务器。
    5. 当客户机不再需要这个文件时，它将提交剩余的更改，并将布局副本返回给元数据服务器，然后关闭文件。

### 2.4 3种存储简介
* pNFS定义了3种存储：文件、块 和对象存储

1. 文件存储：
        
        存储群是由一组NFS服务器组成的，每个文件都跨越所有服务器或服务器的子集，从而使客户机能够同时获取文件的各个部分。
2. 块存储：
        
        存储区域网络”SAN“来实现。通过使用块存储，文件可以被划分为块并分布到不同的驱动器中。块存储layout将文件块映射到物理存储块。
3. 对象存储：
     
        类似于文件存储，但是使用的是对象ID来存取，不是文件句柄，并且文件分割功能更加复杂强大。

以上的pNFS存储技术中，谁是最优秀的呢？这个需要 “依情况而定”。应该采用哪种存储技术由预算、速度、伸缩性、简单性等因素共同决定。

## 3. pNFS实例
本章实例主要为演示pNFS的搭建和挂载使用，如果生产系统要使用，建议一定要多多进行测试。
### 1.2 本章实例图
![](../images/FTP&SMB&NFS/shili.png)

### 1.3 试验环境
|系统版本|磁盘数量|网卡数量|ip地址|主机名称|虚拟化|备注|
|:---|:---|:---|:---|:---|:---|:---|
|CentOS 7.4|1Vdisk|1|192.168.56.101|client|Vbox|client|
|CentOS 7.4|1Vdisk|1|192.168.56.102|pnfs-osd|Vbox|元数据|
|CentOS 7.4|1Vdisk|1|192.168.56.104|pnfs-1|Vbox|Server|
|CentOS 7.4|1Vdisk|1|192.168.56.105|pnfs-2|Vbox|Server|


### 1.4 测试内容和环境说明
1.

### 1.5 安装配置pNFS
1. 从RPM安装
对于使用Redhat/CentOS/Fedora的用户来说，从如下URL下载：
http://fedorapeople.org/~steved/repos/pnfs/


根据OS版本和体系结构选择下载合适的RPMs，其中kernel和nfs-utils安装包是必需的。RPM安装过程中根据提示使用yum或apt-get安装相关的依赖包。kernel安装成功后，需要验证一下/boot和Grub相关信息是否正确，然后reboot机器即可。

 

从源码安装
源码安装相对要比RPM安装复杂许多，但这是很不错的体验，建议搞开发的用户采用这种方式进行安装。Kernel和nfs-utils最新源码可以从如下Git库下载：

[cpp] view plain copy

    git clone git://git.linux-nfs.org/projects/bhalevy/linux-pnfs.git (这个比较大，约1GB)  
    git clone git://linux-nfs.org/~bhalevy/pnfs-nfs-utils.git  


根据内核源码中的Documents/fs/spnfs.txt配置kernel编译选项，make menuconfig时把NFSv41/spnfs的相关选项选中。配置完成后，.config中应该包含以下诸项：
[cpp] view plain copy

    CONFIG_NETWORK_FILESYSTEMS=y  
    CONFIG_NFS_FS=m  
    CONFIG_NFS_V4=y  
    CONFIG_NFS_V4_1=y  
    CONFIG_PNFS=y  
    CONFIG_NFSD=m  
    # CONFIG_PNFSD_LOCAL_EXPORT is not set  
    CONFIG_SPNFS=y  
    CONFIG_SPNFS_LAYOUTSEGMENTS=y  

 

接下来就可以编译和安装内核了

[cpp] view plain copy

    make  
    make modules  
    make modules_install  
    make install  


如果没有生成initramfs，mdinitrd /boot/initramfs-2.6.38-pnfs 2.6.38-pnfs生成。
与RPM安装一样，成功安装kernel后，验证相关信息正确配置后，即可reboot机器。

nfs-utils依赖许多开发库，编译前需要进行安装解决依赖问题。
[cpp] view plain copy

    yum install libtirpc{,-devel} tcp_wrappers{,-devel} libevent{,-devel} nfs-utils-lib{,-devel} libgssglue{,-devel} libnfsidmap{,-devel} libblkid{,-devel} libcap{,-devel} device-mapper-devel  
    sh autogen.sh  
    ./configure --prefix=/usr  
    make  
    make install  

 

配置Data Server
[cpp] view plain copy

    mkdir /export  
    mkdir /export/pnfs  
    vi /etc/export，编辑如下内容：  
    /export/pnfs  *(rw,sync,fsid=0,insecure,no_subtree_check,no_root_squash, pnfs)  
    /etc/init.d/nfs start，启动nfs server。  

每个Data Server均进行如上配置，DS是常规的NFSv4.1服务器。

 

配置Metadata Server
[cpp] view plain copy

    mkdir /export  
    mkdir /export/spnfs  
    mkdir /spnfs  
    mkdir /spnfs/192.168.233.131  (第一个DS mount点)  
    mkdir /spnfs/192.168.233.132  (第二个DS mount点)  

vi /etc/export，编辑如下内容：

[cpp] view plain copy

    /export/spnfs  *(rw,sync,fsid=0,insecure,no_subtree_check,no_root_squash, pnfs)  


vi /etc/spnfsd.conf，编辑如下内容：(可以从nfs-util源码中util/spnfsd/spnfsd.conf复制并修改)

[cpp] view plain copy

    [General]  
    Verbosity = 1  
    Stripe-size = 8192  
    Dense-striping = 0  
    Pipefs-Directory = /var/lib/nfs/rpc_pipefs  
    DS-Mount-Directory = /spnfs  
      
    [DataServers]  
    NumDS = 2  
      
    DS1_IP = 192.168.233.131  
    DS1_PORT = 2049  
    DS1_ROOT = /pnfs  
    DS1_ID = 1  
      
    DS2_IP = 192.168.233.132  
    DS2_PORT = 2049  
    DS2_ROOT = /pnfs  
    DS2_ID = 2  

然后将DS服务器输出目录mount至元MDS上，启动nfs server和spnfs server：
[c-sharp] view plain copy

    mount -t nfs4 -o minorversion=1 192.168.233.131:/pnfs /spnfs/192.168.233.131  
    mount -t nfs4 -o minorversion=1 192.168.233.132:/pnfs /spnfs/192.168.233.132  
    /etc/init.d/nfs start  
    spnfs  

 

配置Client
pNFS支持Block, Object, Files协议，我们这里使用Files协议访问，需要加载nfs_layout_nfsv41_files。
[c-sharp] view plain copy

    modprobe nfs_layout_nfsv41_files  
    mount -t nfs4 -o minorversion=1 192.168.233.130:/ /mnt/pnfs  

 

pNFS测试
cd /mnt/pnfs
dd if=/dev/zero of=f8g bs=4M count=2K
dd中使用top监控DS和MDS系统负载情况，看看IO是不是发生在DS与Client之间。如果MDS I/O负载很高，接近与DS，则安装配置可能有问题。

分别在MDS和DS使用stat查看f8g的属性：
stat /export/spnfs/f8g (MDS上f8g的block数量应该为0，只表现名字空间)
stat /export/pnfs/* (DS1和DS2上f8g对应对象的block数量应该>0，实际存储文件数据)